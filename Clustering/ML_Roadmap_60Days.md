# ğŸ¤– Machine Learning â€” Complete 60-Day Roadmap
### From Absolute Beginner to Job-Ready ML Practitioner

> **Author:** Ashok | **Duration:** 60 Days | **Daily Commitment:** 2â€“3 hours/day

---

## ğŸ“Š Roadmap Overview

```
PHASE 1          PHASE 2          PHASE 3          PHASE 4          PHASE 5
Days 01â€“10       Days 11â€“25       Days 26â€“40       Days 41â€“52       Days 53â€“60
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Foundations  â†’   Supervised   â†’   Unsupervised â†’   Advanced ML  â†’   Projects &
& Math           Learning         + Feature Eng.   & Pipelines       Portfolio
```

---

## ğŸ—“ï¸ Phase-by-Phase Plan

| Phase | Days | Topic | Goal |
|---|---|---|---|
| ğŸ”¢ **Phase 1** | 01 â€“ 10 | Math, Python & ML Foundations | Build the base |
| ğŸ¯ **Phase 2** | 11 â€“ 25 | Supervised Learning | Master core algorithms |
| ğŸ” **Phase 3** | 26 â€“ 40 | Unsupervised Learning + Feature Engineering | Handle real data |
| ğŸš€ **Phase 4** | 41 â€“ 52 | Advanced ML + Model Deployment | Production-ready skills |
| ğŸ† **Phase 5** | 53 â€“ 60 | Capstone Projects + Portfolio | Land the job |

---

## ğŸ“… PHASE 1 â€” Foundations & Math (Days 1â€“10)

> **Goal:** Build the mathematical and Python programming foundation that underpins every ML algorithm.

---

### ğŸ“Œ Day 1â€“2 â€” Python for ML

| Day | Topics | Practice |
|---|---|---|
| **Day 1** | NumPy arrays, broadcasting, vectorisation | 20 NumPy exercises |
| **Day 2** | Pandas DataFrames, indexing, groupby, merge | Load & explore a CSV dataset |

```python
# Day 1 â€” NumPy essentials
import numpy as np

A = np.array([[1, 2], [3, 4]])
print(A.T)              # Transpose
print(np.dot(A, A))     # Matrix multiplication
print(A @ A)            # Same as above
print(np.linalg.inv(A)) # Inverse

# Day 2 â€” Pandas essentials
import pandas as pd

df = pd.read_csv('data.csv')
print(df.describe())                # Summary stats
print(df.isnull().sum())            # Missing values
print(df.groupby('category').mean()) # Group stats
```

**Resources:**
- ğŸ“– [NumPy Quickstart](https://numpy.org/doc/stable/user/quickstart.html)
- ğŸ“– [Pandas 10 Minutes](https://pandas.pydata.org/docs/user_guide/10min.html)

---

### ğŸ“Œ Day 3â€“4 â€” Linear Algebra

| Day | Topics | Why It Matters |
|---|---|---|
| **Day 3** | Vectors, dot product, matrix ops, transpose | Neural networks, PCA |
| **Day 4** | Eigenvalues, eigenvectors, SVD basics | PCA, dimensionality reduction |

**Key Concepts:**

```
Vector Dot Product:    a Â· b = Î£ aáµ¢báµ¢
Matrix Multiply:       C = A @ B   (rows Ã— cols)
Eigendecomposition:    A Â· v = Î» Â· v
```

---

### ğŸ“Œ Day 5 â€” Statistics & Probability

| Topic | Key Formula | Use in ML |
|---|---|---|
| Mean / Variance | Î¼ = Î£x/n, ÏƒÂ² = Î£(xâˆ’Î¼)Â²/n | Feature scaling |
| Bayes' Theorem | P(A\|B) = P(B\|A)Â·P(A) / P(B) | Naive Bayes |
| Normal Distribution | f(x) = (1/Ïƒâˆš2Ï€) e^(âˆ’(xâˆ’Î¼)Â²/2ÏƒÂ²) | Assumptions in LR |
| Correlation | Ï = Cov(X,Y) / (Ïƒâ‚“Ïƒáµ§) | Feature selection |
| Central Limit Theorem | Sample means â†’ Normal | Confidence intervals |

---

### ğŸ“Œ Day 6 â€” Calculus for ML

| Topic | Key Concept | Use in ML |
|---|---|---|
| Derivatives | Rate of change of a function | Gradient descent |
| Partial Derivatives | âˆ‚f/âˆ‚x holding other vars constant | Backpropagation |
| Chain Rule | d/dx[f(g(x))] = f'(g(x))Â·g'(x) | Neural networks |
| Gradient | Vector of all partial derivatives | Optimisation |

```
Gradient Descent update rule:
Î¸ = Î¸ âˆ’ Î± Â· âˆ‡J(Î¸)

Where:  Î±  = learning rate
        âˆ‡J = gradient of loss function
```

---

### ğŸ“Œ Day 7 â€” Data Visualisation

| Library | Use Case | Key Plots |
|---|---|---|
| **Matplotlib** | Custom plots | Line, scatter, histogram |
| **Seaborn** | Statistical plots | Heatmap, pairplot, boxplot |
| **Plotly** | Interactive plots | Dashboard-ready charts |

```python
import seaborn as sns
import matplotlib.pyplot as plt

# Correlation heatmap
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.show()

# Distribution plot
sns.histplot(df['feature'], kde=True)
plt.show()

# Pairplot
sns.pairplot(df, hue='target')
plt.show()
```

---

### ğŸ“Œ Day 8â€“9 â€” ML Fundamentals

| Day | Topics |
|---|---|
| **Day 8** | What is ML? Supervised vs Unsupervised vs Reinforcement |
| **Day 8** | Train/Validation/Test split, Overfitting vs Underfitting |
| **Day 9** | Loss functions, Gradient Descent (Batch, SGD, Mini-batch) |
| **Day 9** | Bias-Variance tradeoff, Regularisation (L1, L2) |

```
Bias-Variance Tradeoff:
Total Error = BiasÂ² + Variance + Irreducible Noise

High Bias   â†’ Underfitting (model too simple)
High Variance â†’ Overfitting (model too complex)
```

**Loss Functions Summary:**

| Loss | Formula | Used In |
|---|---|---|
| MSE | (1/n) Î£(y âˆ’ Å·)Â² | Regression |
| MAE | (1/n) Î£\|y âˆ’ Å·\| | Robust regression |
| Binary Cross-Entropy | âˆ’[yÂ·log(Å·) + (1âˆ’y)Â·log(1âˆ’Å·)] | Classification |
| Categorical Cross-Entropy | âˆ’Î£ yáµ¢Â·log(Å·áµ¢) | Multi-class |
| Hinge Loss | max(0, 1 âˆ’ yÂ·Å·) | SVM |

---

### ğŸ“Œ Day 10 â€” Scikit-Learn Workflow

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score

# Universal ML workflow in sklearn
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

pipe = Pipeline([
    ('scaler', StandardScaler()),
    ('model',  YourModel())
])

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
```

### âœ… Phase 1 Checkpoint

- [ ] Can manipulate arrays and DataFrames confidently
- [ ] Understand dot products, matrix multiplication, eigenvalues
- [ ] Know what loss functions, gradients, and bias-variance mean
- [ ] Have run a basic sklearn pipeline end-to-end

---

## ğŸ“… PHASE 2 â€” Supervised Learning (Days 11â€“25)

> **Goal:** Master every major supervised learning algorithm â€” both regression and classification.

---

### ğŸ“Œ Day 11â€“12 â€” Linear & Polynomial Regression

| Day | Topics |
|---|---|
| **Day 11** | Simple Linear Regression, OLS, RÂ², RMSE, MAE |
| **Day 12** | Multiple Linear Regression, Polynomial Regression, assumptions |

```
Linear Regression:   Å· = Î¸â‚€ + Î¸â‚xâ‚ + Î¸â‚‚xâ‚‚ + ... + Î¸â‚™xâ‚™
OLS Solution:        Î¸ = (Xáµ€X)â»Â¹ Xáµ€y
```

**Regression Metrics:**

| Metric | Formula | Range | Better |
|---|---|---|---|
| RÂ² Score | 1 âˆ’ SS_res/SS_tot | 0 to 1 | Higher |
| RMSE | âˆš(Î£(yâˆ’Å·)Â²/n) | 0 â†’ âˆ | Lower |
| MAE | Î£\|yâˆ’Å·\|/n | 0 â†’ âˆ | Lower |
| MAPE | (1/n)Î£\|yâˆ’Å·\|/\|y\| Ã— 100% | 0 â†’ âˆ | Lower |

---

### ğŸ“Œ Day 13 â€” Ridge, Lasso & ElasticNet

| Model | Penalty | Effect | Use When |
|---|---|---|---|
| **Ridge (L2)** | Î»Î£Î¸Â² | Shrinks all coefficients | Many small features |
| **Lasso (L1)** | Î»Î£\|Î¸\| | Sets some to zero (sparse) | Feature selection |
| **ElasticNet** | Î»â‚Î£\|Î¸\| + Î»â‚‚Î£Î¸Â² | Mix of both | Many correlated features |

```python
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.model_selection import GridSearchCV

# Ridge with CV tuning
ridge = Ridge()
params = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}
grid = GridSearchCV(ridge, params, cv=5, scoring='r2')
grid.fit(X_train, y_train)
print(f"Best alpha: {grid.best_params_}")
```

---

### ğŸ“Œ Day 14â€“15 â€” Logistic Regression

| Day | Topics |
|---|---|
| **Day 14** | Sigmoid function, binary classification, decision boundary |
| **Day 15** | Multi-class: One-vs-Rest, One-vs-One, Softmax |

```
Sigmoid:     Ïƒ(z) = 1 / (1 + eâ»á¶»)
Prediction:  Å· = Ïƒ(Î¸áµ€x)  â†’  class 1 if Å· â‰¥ 0.5
```

**Classification Metrics:**

| Metric | Formula | Focus |
|---|---|---|
| Accuracy | (TP+TN)/(TP+TN+FP+FN) | Overall correctness |
| Precision | TP/(TP+FP) | Avoid false positives |
| Recall | TP/(TP+FN) | Avoid false negatives |
| F1 Score | 2Â·PÂ·R/(P+R) | Balance P and R |
| ROC-AUC | Area under ROC curve | Rank-ordering |

---

### ğŸ“Œ Day 16 â€” Decision Trees

```
                  [Feature A â‰¤ 3.5]
                 /                  \
         [Feature B â‰¤ 1.0]     [Feature C â‰¤ 2.5]
         /           \           /            \
      Class 0      Class 1   Class 1        Class 0
```

| Concept | Detail |
|---|---|
| **Split criterion** | Gini Impurity or Information Gain (Entropy) |
| **Gini** | 1 âˆ’ Î£ páµ¢Â² (lower = purer) |
| **Entropy** | âˆ’Î£ páµ¢Â·logâ‚‚(páµ¢) |
| **Pruning** | max_depth, min_samples_split, min_samples_leaf |
| **Pros** | Interpretable, handles mixed types, no scaling needed |
| **Cons** | High variance, prone to overfitting |

---

### ğŸ“Œ Day 17â€“18 â€” Ensemble Methods

#### Random Forest (Day 17)

```
                    Random Forest
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      Tree 1         Tree 2         Tree N
      (subset)       (subset)       (subset)
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   Majority Vote / Avg
```

| Parameter | Effect |
|---|---|
| `n_estimators` | More trees = more stable (diminishing returns) |
| `max_features` | Subset of features per split (controls correlation) |
| `max_depth` | Depth of each tree |
| `bootstrap` | Whether to sample with replacement |

#### Boosting (Day 18)

| Algorithm | Key Idea | Best For |
|---|---|---|
| **AdaBoost** | Upweight misclassified samples | Binary classification |
| **Gradient Boosting** | Fit residuals sequentially | Tabular regression/classification |
| **XGBoost** | Regularised GBM with speed | Kaggle competitions |
| **LightGBM** | Leaf-wise tree growth, fast | Large datasets |
| **CatBoost** | Native categorical handling | Mixed-type data |

```python
from xgboost import XGBClassifier

xgb = XGBClassifier(
    n_estimators=300,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
xgb.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=50)
```

---

### ğŸ“Œ Day 19â€“20 â€” Support Vector Machines (SVM)

| Day | Topics |
|---|---|
| **Day 19** | Hard-margin SVM, support vectors, hyperplane, margin maximisation |
| **Day 20** | Soft-margin (C), Kernel trick (RBF, Polynomial, Sigmoid) |

```
Decision boundary:   wáµ€x + b = 0
Margin:              2 / â€–wâ€–
Objective:           Minimise Â½â€–wâ€–Â²  subject to  yáµ¢(wáµ€xáµ¢ + b) â‰¥ 1
```

| Kernel | Formula | Use Case |
|---|---|---|
| Linear | K(x,z) = xáµ€z | Linearly separable data |
| RBF | K(x,z) = exp(âˆ’Î³â€–xâˆ’zâ€–Â²) | Non-linear, general purpose |
| Polynomial | K(x,z) = (xáµ€z + c)^d | Image features |

---

### ğŸ“Œ Day 21 â€” K-Nearest Neighbours (KNN)

| Aspect | Detail |
|---|---|
| **Idea** | Classify by majority vote of K nearest neighbours |
| **Distance** | Euclidean, Manhattan, Minkowski |
| **Choosing K** | Cross-validate; small K = high variance, large K = high bias |
| **Scaling** | Mandatory â€” KNN is distance-based |
| **Complexity** | O(nÂ·d) per prediction â€” slow on large data |

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score

errors = []
for k in range(1, 31):
    knn = KNeighborsClassifier(n_neighbors=k)
    score = cross_val_score(knn, X_train, y_train, cv=5).mean()
    errors.append(score)

best_k = errors.index(max(errors)) + 1
print(f"Best K: {best_k}")
```

---

### ğŸ“Œ Day 22 â€” Naive Bayes

| Variant | Assumption | Best For |
|---|---|---|
| **GaussianNB** | Features ~ Normal distribution | Continuous features |
| **MultinomialNB** | Features are counts | Text classification |
| **BernoulliNB** | Features are binary | Spam detection |

```
Bayes Rule:  P(class | x) âˆ P(x | class) Â· P(class)
Naive:       assumes features are conditionally independent
```

---

### ğŸ“Œ Day 23â€“24 â€” Model Evaluation & Hyperparameter Tuning

#### Cross-Validation (Day 23)

| Technique | Description | Best For |
|---|---|---|
| **K-Fold CV** | Split into K folds, rotate test set | General purpose |
| **Stratified K-Fold** | Preserves class ratio in each fold | Imbalanced classes |
| **Leave-One-Out** | K = n (every point is test once) | Very small datasets |
| **Time-Series Split** | No future data leaks into training | Time-series data |

#### Hyperparameter Tuning (Day 24)

| Method | Strategy | Speed | Best For |
|---|---|---|---|
| **GridSearchCV** | Exhaustive search | Slow | Small param space |
| **RandomizedSearchCV** | Random sampling | Fast | Large param space |
| **Optuna / Bayesian** | Smart sequential search | Fastest | Complex models |

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

params = {
    'n_estimators': randint(100, 500),
    'max_depth':    randint(3, 15),
    'learning_rate': uniform(0.01, 0.3)
}

search = RandomizedSearchCV(model, params, n_iter=50,
                             cv=5, scoring='f1', random_state=42)
search.fit(X_train, y_train)
print(search.best_params_)
```

---

### ğŸ“Œ Day 25 â€” Handling Imbalanced Datasets

| Technique | Method | Library |
|---|---|---|
| **Oversampling** | SMOTE â€” synthetic minority samples | `imbalanced-learn` |
| **Undersampling** | Remove majority class samples | `imbalanced-learn` |
| **Class Weights** | `class_weight='balanced'` in sklearn | Built-in |
| **Threshold Tuning** | Adjust decision threshold (not 0.5) | Manual / sklearn |
| **Evaluation** | Always use F1, Precision-Recall AUC | Never accuracy alone |

### âœ… Phase 2 Checkpoint

- [ ] Can implement all algorithms from scratch conceptually
- [ ] Understand when to use each algorithm
- [ ] Can tune hyperparameters using GridSearch / RandomSearch
- [ ] Know all classification & regression evaluation metrics

---

## ğŸ“… PHASE 3 â€” Unsupervised Learning + Feature Engineering (Days 26â€“40)

> **Goal:** Handle real-world messy data and discover hidden structure without labels.

---

### ğŸ“Œ Day 26â€“28 â€” Clustering Algorithms

| Day | Algorithm | Key Concept |
|---|---|---|
| **Day 26** | K-Means | Centroid-based, minimise WCSS |
| **Day 27** | DBSCAN | Density-based, handles noise & arbitrary shapes |
| **Day 28** | Hierarchical Clustering | Dendrogram, no K needed upfront |

#### K-Means Quick Reference

```
Objective:     J = Î£â‚– Î£áµ¢ â€–xáµ¢ âˆ’ Î¼â‚–â€–Â²
Choose K:      Elbow method (WCSS) + Silhouette score
Init:          K-Means++ (sklearn default)
Metrics:       Silhouette, Calinski-Harabasz, Davies-Bouldin
```

#### DBSCAN Parameters

| Parameter | Effect |
|---|---|
| `eps` | Neighbourhood radius â€” smaller = tighter clusters |
| `min_samples` | Min points to form a core point |
| Points labeled `-1` | Noise / outliers |

#### Hierarchical Clustering

| Linkage | Merges clusters by... |
|---|---|
| Single | Min distance between any two points |
| Complete | Max distance between any two points |
| Average | Average distance between all point pairs |
| Ward | Minimises within-cluster variance (most common) |

---

### ğŸ“Œ Day 29â€“30 â€” Dimensionality Reduction

| Day | Algorithm | Use Case |
|---|---|---|
| **Day 29** | PCA (Principal Component Analysis) | Linear reduction, visualisation |
| **Day 30** | t-SNE, UMAP | Non-linear, high-dim visualisation |

#### PCA Explained

```
Steps:
1. Standardise the data
2. Compute covariance matrix: C = Xáµ€X / (nâˆ’1)
3. Compute eigenvectors & eigenvalues of C
4. Sort by eigenvalue (variance explained)
5. Project data onto top K eigenvectors

Explained Variance Ratio: how much variance each PC captures
Cumulative Variance:      choose K where sum â‰¥ 95%
```

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=0.95)  # Keep 95% variance
X_reduced = pca.fit_transform(X_scaled)
print(f"Original: {X.shape[1]} â†’ Reduced: {X_reduced.shape[1]} features")
print(pca.explained_variance_ratio_)
```

| Method | Type | Preserves | Best For |
|---|---|---|---|
| **PCA** | Linear | Global variance structure | Preprocessing, speed |
| **t-SNE** | Non-linear | Local neighbourhood | 2D/3D visualisation |
| **UMAP** | Non-linear | Both local & global | Faster than t-SNE |

---

### ğŸ“Œ Day 31â€“35 â€” Feature Engineering

> This is where most ML competitions are won or lost.

#### Day 31 â€” Handling Missing Values

| Strategy | When to Use | Code |
|---|---|---|
| **Drop rows** | < 5% missing | `df.dropna()` |
| **Mean/Median fill** | Numerical, MAR | `SimpleImputer(strategy='median')` |
| **Mode fill** | Categorical | `SimpleImputer(strategy='most_frequent')` |
| **KNN Imputer** | Correlated features | `KNNImputer(n_neighbors=5)` |
| **Iterative Imputer** | Complex patterns | `IterativeImputer()` |

#### Day 32 â€” Encoding Categorical Variables

| Technique | When | Code |
|---|---|---|
| **Label Encoding** | Ordinal categories | `LabelEncoder()` |
| **One-Hot Encoding** | Nominal, low cardinality | `pd.get_dummies()` |
| **Target Encoding** | High cardinality | `category_encoders.TargetEncoder()` |
| **Frequency Encoding** | High cardinality | `df['col'].map(df['col'].value_counts())` |

#### Day 33 â€” Feature Scaling

| Scaler | Formula | Use When |
|---|---|---|
| **StandardScaler** | (x âˆ’ Î¼) / Ïƒ | Normal-ish data, SVM, LR |
| **MinMaxScaler** | (x âˆ’ min)/(max âˆ’ min) | Neural networks, KNN |
| **RobustScaler** | (x âˆ’ median) / IQR | Data with outliers |
| **Log Transform** | log(x + 1) | Right-skewed distributions |

#### Day 34 â€” Feature Creation & Selection

**Creation:**
```python
# Interaction features
df['area'] = df['length'] * df['width']

# Ratio features
df['price_per_sqft'] = df['price'] / df['sqft']

# Date features
df['day_of_week'] = df['date'].dt.dayofweek
df['month']       = df['date'].dt.month
df['is_weekend']  = df['day_of_week'].isin([5, 6]).astype(int)
```

**Selection Methods:**

| Method | Type | Code |
|---|---|---|
| Correlation filter | Filter | `df.corr()` |
| SelectKBest | Filter | `SelectKBest(f_classif, k=10)` |
| Recursive Feature Elimination | Wrapper | `RFE(estimator, n_features_to_select=10)` |
| Feature Importance | Embedded | `model.feature_importances_` |
| L1 Regularisation | Embedded | `SelectFromModel(Lasso())` |

#### Day 35 â€” Outlier Detection & Treatment

| Method | Approach | Code |
|---|---|---|
| **IQR Method** | Remove points outside Q1âˆ’1.5Ã—IQR / Q3+1.5Ã—IQR | Manual |
| **Z-Score** | Remove points where \|z\| > 3 | `scipy.stats.zscore` |
| **Isolation Forest** | Anomaly score via random trees | `IsolationForest()` |
| **Local Outlier Factor** | Density-based anomaly score | `LocalOutlierFactor()` |

---

### ğŸ“Œ Day 36â€“38 â€” Association Rules & Advanced Unsupervised

| Day | Topic | Key Algorithm |
|---|---|---|
| **Day 36** | Market Basket Analysis | Apriori, FP-Growth |
| **Day 37** | Anomaly Detection | Isolation Forest, Autoencoder |
| **Day 38** | Gaussian Mixture Models | Soft clustering, EM algorithm |

#### Association Rules Metrics

| Metric | Meaning | Formula |
|---|---|---|
| **Support** | How often itemset appears | P(A âˆ© B) |
| **Confidence** | How often rule is correct | P(B\|A) |
| **Lift** | Strength above chance | Conf / P(B) |

---

### ğŸ“Œ Day 39â€“40 â€” Pipelines & Preprocessing Automation

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer

# Define column types
num_cols = ['age', 'salary', 'experience']
cat_cols = ['department', 'city']

# Numerical pipeline
num_pipe = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler',  StandardScaler())
])

# Categorical pipeline
cat_pipe = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])

# Combine
preprocessor = ColumnTransformer([
    ('num', num_pipe, num_cols),
    ('cat', cat_pipe, cat_cols)
])

# Full pipeline with model
full_pipe = Pipeline([
    ('preprocessor', preprocessor),
    ('model',        XGBClassifier())
])

full_pipe.fit(X_train, y_train)
```

### âœ… Phase 3 Checkpoint

- [ ] Can cluster data and choose the right algorithm for the task
- [ ] Understand PCA and when to use t-SNE vs UMAP
- [ ] Can engineer features from raw data (dates, text, categoricals)
- [ ] Can build a clean sklearn Pipeline with ColumnTransformer

---

## ğŸ“… PHASE 4 â€” Advanced ML & Deployment (Days 41â€“52)

> **Goal:** Learn cutting-edge techniques and build production-ready skills.

---

### ğŸ“Œ Day 41â€“44 â€” Neural Networks & Deep Learning Basics

| Day | Topics |
|---|---|
| **Day 41** | Perceptron, activation functions, forward pass |
| **Day 42** | Backpropagation, optimisers (SGD, Adam, RMSProp) |
| **Day 43** | Build ANN with Keras/TensorFlow for classification & regression |
| **Day 44** | Dropout, Batch Normalisation, Early Stopping |

**Activation Functions:**

| Function | Formula | Use |
|---|---|---|
| ReLU | max(0, x) | Hidden layers (default) |
| Sigmoid | 1/(1+eâ»Ë£) | Binary output |
| Softmax | eË£áµ¢/Î£eË£â±¼ | Multi-class output |
| Tanh | (eË£âˆ’eâ»Ë£)/(eË£+eâ»Ë£) | RNNs, hidden layers |
| Leaky ReLU | max(0.01x, x) | Dying ReLU problem |

```python
import tensorflow as tf
from tensorflow import keras

model = keras.Sequential([
    keras.layers.Dense(128, activation='relu', input_shape=(n_features,)),
    keras.layers.Dropout(0.3),
    keras.layers.BatchNormalization(),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=50, batch_size=32,
          validation_split=0.2,
          callbacks=[keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)])
```

---

### ğŸ“Œ Day 45â€“46 â€” Natural Language Processing (NLP) Basics

| Day | Topics |
|---|---|
| **Day 45** | Text preprocessing: tokenisation, stopwords, stemming, lemmatisation |
| **Day 45** | Bag of Words, TF-IDF, n-grams |
| **Day 46** | Word embeddings: Word2Vec, GloVe |
| **Day 46** | Sentiment analysis, text classification pipeline |

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

text_pipe = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1,2))),
    ('clf',   LogisticRegression(max_iter=1000))
])

text_pipe.fit(X_train_text, y_train)
print(f"Test Accuracy: {text_pipe.score(X_test_text, y_test):.4f}")
```

---

### ğŸ“Œ Day 47 â€” Time Series Forecasting

| Concept | Detail |
|---|---|
| **Stationarity** | Constant mean/variance over time (ADF test) |
| **ACF / PACF** | Identify AR and MA orders |
| **ARIMA** | Auto-Regressive Integrated Moving Average |
| **SARIMA** | ARIMA + seasonal component |
| **Prophet** | Facebook's time-series library |
| **ML for TS** | Lag features â†’ XGBoost / LightGBM |

```python
from statsmodels.tsa.arima.model import ARIMA

model = ARIMA(train_series, order=(2, 1, 2))
fitted = model.fit()
forecast = fitted.forecast(steps=30)
```

---

### ğŸ“Œ Day 48 â€” Recommendation Systems

| Type | Approach | Example |
|---|---|---|
| **Content-Based** | Item features similarity | "Because you watched X" |
| **Collaborative Filtering** | User-item matrix factorisation | "Users like you watched Y" |
| **Hybrid** | Content + Collaborative | Netflix, Spotify |

---

### ğŸ“Œ Day 49â€“50 â€” ML Experiment Tracking & MLOps Basics

| Tool | Purpose |
|---|---|
| **MLflow** | Track experiments, log metrics, register models |
| **Weights & Biases** | Visual experiment tracking |
| **DVC** | Data version control |
| **Great Expectations** | Data validation & testing |

```python
import mlflow
import mlflow.sklearn

with mlflow.start_run():
    mlflow.log_param("n_estimators", 300)
    mlflow.log_param("max_depth", 6)

    model.fit(X_train, y_train)
    acc = accuracy_score(y_test, model.predict(X_test))

    mlflow.log_metric("accuracy", acc)
    mlflow.sklearn.log_model(model, "model")
    print(f"Run ID: {mlflow.active_run().info.run_id}")
```

---

### ğŸ“Œ Day 51â€“52 â€” Model Deployment

| Method | Tool | Use Case |
|---|---|---|
| **REST API** | FastAPI + Uvicorn | Production APIs |
| **Containerisation** | Docker | Portable deployment |
| **Cloud Deployment** | AWS SageMaker, GCP AI Platform | Scalable serving |
| **Serverless** | AWS Lambda | Lightweight inference |

```python
# FastAPI deployment example
from fastapi import FastAPI
import joblib
import numpy as np

app = FastAPI()
model = joblib.load("model.pkl")

@app.post("/predict")
def predict(features: list[float]):
    X = np.array(features).reshape(1, -1)
    prediction = model.predict(X)[0]
    probability = model.predict_proba(X)[0].max()
    return {"prediction": int(prediction), "confidence": float(probability)}
```

### âœ… Phase 4 Checkpoint

- [ ] Can build and train a neural network with Keras
- [ ] Can preprocess and classify text data
- [ ] Can log and track ML experiments with MLflow
- [ ] Can deploy a model as a REST API with FastAPI

---

## ğŸ“… PHASE 5 â€” Capstone Projects & Portfolio (Days 53â€“60)

> **Goal:** Build 3 real-world projects and create a portfolio that gets you hired.

---

### ğŸ“Œ Day 53â€“55 â€” Project 1: Structured Data (Tabular)

**ğŸ  House Price Prediction (Regression)**

```
Dataset:    Kaggle House Prices (Ames Housing)
Goal:       Predict sale price from 80 features
Techniques: Feature engineering, XGBoost, Stacking
Metric:     RMSLE (Root Mean Squared Log Error)

Steps:
1. EDA â€” distributions, correlations, outliers
2. Feature engineering â€” year diffs, area ratios
3. Encode categoricals â€” target encoding for high-cardinality
4. Model â€” XGBoost + LightGBM + Ridge stacking
5. Tune â€” Optuna / RandomizedSearchCV
6. Evaluate â€” CV RMSLE, feature importance plot
7. Submit â€” Generate predictions, document findings
```

---

### ğŸ“Œ Day 56â€“57 â€” Project 2: Classification

**ğŸ’³ Credit Card Fraud Detection (Imbalanced Classification)**

```
Dataset:    Kaggle Credit Card Fraud (284K transactions, 0.17% fraud)
Goal:       Detect fraudulent transactions
Techniques: SMOTE, class weights, threshold tuning
Metric:     Precision-Recall AUC, F1

Steps:
1. EDA â€” severe class imbalance analysis
2. Resample â€” SMOTE + Tomek Links
3. Model â€” Isolation Forest + XGBoost + Logistic Regression
4. Tune threshold â€” maximise F1 / Recall tradeoff
5. Evaluate â€” Confusion matrix, PR-AUC curve
6. Deploy â€” FastAPI endpoint with risk scoring
```

---

### ğŸ“Œ Day 58â€“59 â€” Project 3: NLP or Clustering

**Choose One:**

```
Option A â€” Sentiment Analysis
   Dataset:    IMDB / Amazon Reviews
   Goal:       Classify reviews as positive/negative
   Techniques: TF-IDF + LR, Word2Vec + LSTM
   Metric:     F1 Score, ROC-AUC

Option B â€” Customer Segmentation
   Dataset:    Mall Customers / RFM data
   Goal:       Segment customers for marketing
   Techniques: K-Means, DBSCAN, PCA visualisation
   Metric:     Silhouette Score, business interpretation
```

---

### ğŸ“Œ Day 60 â€” Portfolio & Job Readiness

**GitHub Portfolio Checklist:**

- [ ] Each project has a clean `README.md` with problem statement, approach, results
- [ ] Notebooks are well-commented with markdown explanations
- [ ] Include visualisations (confusion matrix, feature importance, cluster plots)
- [ ] Add a `requirements.txt` or `environment.yml` to every project
- [ ] Pin your best 3â€“4 repos on your GitHub profile

**LinkedIn / Resume:**
- [ ] List all algorithms you can implement and explain
- [ ] Quantify results: "Achieved 94.2% F1-score on imbalanced fraud dataset"
- [ ] Link to GitHub and deployed projects

---

## ğŸ“‹ Complete Day-by-Day Schedule

| Day | Phase | Topic | Deliverable |
|---|---|---|---|
| 01 | 1 | NumPy | 20 NumPy exercises |
| 02 | 1 | Pandas | EDA on a real dataset |
| 03 | 1 | Linear Algebra | Matrix ops notebook |
| 04 | 1 | Eigenvalues & SVD | PCA from scratch |
| 05 | 1 | Statistics & Probability | Stats summary sheet |
| 06 | 1 | Calculus & Gradient Descent | GD from scratch |
| 07 | 1 | Visualisation | Matplotlib + Seaborn plots |
| 08 | 1 | ML Fundamentals | Bias-variance notebook |
| 09 | 1 | Loss Functions & Optimisation | Loss comparison |
| 10 | 1 | Sklearn Workflow | First end-to-end pipeline |
| 11 | 2 | Linear Regression | House price mini-project |
| 12 | 2 | Polynomial & Multiple Regression | Non-linear fitting |
| 13 | 2 | Ridge, Lasso, ElasticNet | Regularisation comparison |
| 14 | 2 | Logistic Regression | Binary classifier |
| 15 | 2 | Multi-class Classification | Iris / Wine dataset |
| 16 | 2 | Decision Trees | Tree visualisation |
| 17 | 2 | Random Forest | Feature importance analysis |
| 18 | 2 | XGBoost & Boosting | Titanic survival |
| 19 | 2 | SVM â€” Linear | Hard margin SVM |
| 20 | 2 | SVM â€” Kernel | RBF kernel non-linear |
| 21 | 2 | KNN | Optimal K search |
| 22 | 2 | Naive Bayes | Spam classifier |
| 23 | 2 | Cross-Validation | Stratified K-Fold |
| 24 | 2 | Hyperparameter Tuning | Optuna / RandomizedSearch |
| 25 | 2 | Imbalanced Data | SMOTE + threshold tuning |
| 26 | 3 | K-Means | Elbow + Silhouette |
| 27 | 3 | DBSCAN | Cluster arbitrary shapes |
| 28 | 3 | Hierarchical Clustering | Dendrogram |
| 29 | 3 | PCA | Dimensionality reduction |
| 30 | 3 | t-SNE & UMAP | High-dim visualisation |
| 31 | 3 | Missing Value Imputation | KNN Imputer |
| 32 | 3 | Categorical Encoding | Target encoding |
| 33 | 3 | Feature Scaling | Scaler comparison |
| 34 | 3 | Feature Selection | RFE + SHAP |
| 35 | 3 | Outlier Detection | Isolation Forest |
| 36 | 3 | Association Rules | Market basket |
| 37 | 3 | Anomaly Detection | Credit card anomalies |
| 38 | 3 | Gaussian Mixture Models | Soft clustering |
| 39 | 3 | Sklearn Pipelines | Full pipeline |
| 40 | 3 | ColumnTransformer | Automated preprocessing |
| 41 | 4 | Perceptron & ANN | Forward pass |
| 42 | 4 | Backpropagation | Manual BP |
| 43 | 4 | Keras ANN | Classification + Regression |
| 44 | 4 | Regularisation in DL | Dropout + BN |
| 45 | 4 | NLP Basics | TF-IDF pipeline |
| 46 | 4 | Word Embeddings | Word2Vec |
| 47 | 4 | Time Series | ARIMA + Prophet |
| 48 | 4 | Recommender Systems | Collaborative filtering |
| 49 | 4 | MLflow Tracking | Log experiments |
| 50 | 4 | Model Versioning | MLflow Model Registry |
| 51 | 4 | FastAPI Deployment | REST API |
| 52 | 4 | Docker | Containerise model |
| 53 | 5 | Project 1 â€” EDA | House Prices EDA |
| 54 | 5 | Project 1 â€” Model | XGBoost + Stacking |
| 55 | 5 | Project 1 â€” Polish | README + Notebook |
| 56 | 5 | Project 2 â€” EDA | Fraud detection EDA |
| 57 | 5 | Project 2 â€” Model | SMOTE + Threshold |
| 58 | 5 | Project 3 â€” Build | NLP or Clustering |
| 59 | 5 | Project 3 â€” Deploy | FastAPI endpoint |
| 60 | 5 | Portfolio Day | GitHub + LinkedIn |

---

## ğŸ› ï¸ Tools & Libraries Stack

| Category | Tools |
|---|---|
| **Language** | Python 3.10+ |
| **Core ML** | Scikit-learn, XGBoost, LightGBM, CatBoost |
| **Deep Learning** | TensorFlow / Keras, PyTorch (optional) |
| **Data** | NumPy, Pandas, SciPy |
| **Visualisation** | Matplotlib, Seaborn, Plotly |
| **NLP** | NLTK, SpaCy, HuggingFace Transformers |
| **Time Series** | Statsmodels, Prophet, sktime |
| **Feature Eng.** | Feature-engine, Category Encoders |
| **Imbalanced** | imbalanced-learn |
| **MLOps** | MLflow, DVC, Weights & Biases |
| **Deployment** | FastAPI, Docker, Streamlit |
| **Notebooks** | Jupyter Lab, Google Colab |

---

## ğŸ“š Learning Resources

| Resource | Type | Best For |
|---|---|---|
| [Hands-On ML (GÃ©ron)](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) | Book | Comprehensive theory + practice |
| [Kaggle Learn](https://www.kaggle.com/learn) | Free Course | Practical ML fast |
| [StatQuest (YouTube)](https://www.youtube.com/@statquest) | Video | Intuitive algorithm explanations |
| [fast.ai](https://www.fast.ai/) | Free Course | Deep learning top-down |
| [ML Mastery](https://machinelearningmastery.com/) | Blog | Code-first tutorials |
| [Papers With Code](https://paperswithcode.com/) | Research | SOTA methods & benchmarks |

---

## âš¡ Daily Study Template

```
ğŸ“… Day XX â€” [Topic Name]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â° Time Block:    2.5 hours

ğŸ“– Theory        (45 min)
   â†’ Read concept, formulas, intuition

ğŸ’» Implementation (60 min)
   â†’ Code from scratch or adapt example

ğŸ§ª Experiment    (30 min)
   â†’ Try on a real dataset (Kaggle / sklearn toy)

ğŸ“ Notes         (15 min)
   â†’ Write key takeaways in your own words

ğŸ” Review        (10 min)
   â†’ Revisit yesterday's notes briefly
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

---

## ğŸ How to Run Projects

```bash
# Clone this repository
git clone https://github.com/ashok/ml-roadmap-60days.git
cd ml-roadmap-60days

# Create virtual environment
python -m venv ml_env
source ml_env/bin/activate      # Linux/Mac
ml_env\Scripts\activate.bat     # Windows

# Install dependencies
pip install -r requirements.txt

# Launch Jupyter
jupyter lab
```

---

## ğŸ“ Repository Structure

```
ml-roadmap-60days/
â”‚
â”œâ”€â”€ phase1_foundations/
â”‚   â”œâ”€â”€ day01_numpy.ipynb
â”‚   â”œâ”€â”€ day02_pandas.ipynb
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ phase2_supervised/
â”‚   â”œâ”€â”€ day11_linear_regression.ipynb
â”‚   â”œâ”€â”€ day17_random_forest.ipynb
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ phase3_unsupervised/
â”‚   â”œâ”€â”€ day26_kmeans.ipynb
â”‚   â”œâ”€â”€ day29_pca.ipynb
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ phase4_advanced/
â”‚   â”œâ”€â”€ day41_neural_networks.ipynb
â”‚   â”œâ”€â”€ day51_fastapi_deployment/
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ phase5_projects/
â”‚   â”œâ”€â”€ project1_house_prices/
â”‚   â”œâ”€â”€ project2_fraud_detection/
â”‚   â””â”€â”€ project3_nlp_or_clustering/
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

<div align="center">

**ğŸ¯ You've got this! Consistency beats intensity â€” 2 hours every day for 60 days = 120 hours of focused ML learning.**

*Made with â¤ï¸ by Ashok | Unsupervised Machine Learning Series*

</div>
